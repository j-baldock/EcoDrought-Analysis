---
title: "Explore Sub-Daily Data"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, cache.lazy = FALSE)
```

Purpose: Explore data at sub-daily temporal resolutions (e.g., 15-min and 1-hour time steps) and compare outputs to daily data. 

Given that streamflow can change so quickly in small, headwater streams, are we missing a key part of the story by using flow data summarized as daily means? Using daily mean flow reduces the range of values, particularly at the upper end (i.e., high flows), and so we may be overlooking the g~G relationship at very high flows. (Note limited analysis of 15-min data as Montana and Wyoming data is collected at the hourly timescale).

* Visualize 15-min and 1-hour data and note diversity in timing of peak flows during events; compare to daily data.
* Use event pairing to explore the mean and variation in time lags between peak flows at Big G and little g's; compare hourly and daily data
* Fit basic wedge model to the 1-hour *unaligned* data...do the results change so much that we need to align time series when working with sub-daily data? Compare to daily data.
* Explore use of dynamic time warping to align time series data


```{r echo=FALSE, message=FALSE}
library(tidyverse)
library(sf)
library(mapview)
library(knitr)
library(dygraphs)
library(hydroEvents)
library(zoo)
library(dataRetrieval)
library(ggpubr)
library(dtw)
```

## Data

### Load data

Bring in site info and sub-daily data
```{r}
# site information and locations
siteinfo <- read_csv("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/EcoDrought_SiteInformation.csv")
siteinfo_sp <- st_as_sf(siteinfo, coords = c("long", "lat"), crs = 4326)
mapview(siteinfo_sp, zcol = "designation")

# flow/yield (and temp) data 
dat_sub <- read_csv("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/EcoDrought_FlowTempData_Raw_ECODandNWIS.csv") 

dat_little <- dat_sub %>% 
  filter(site_name %in% c("West Brook Lower", "Mitchell Brook", "Jimmy Brook", "Obear Brook Lower", "West Brook Upper", "West Brook Reservoir", "Sanderson Brook", "Avery Brook", "West Whately Brook")) %>% 
  select(site_name, datetime, flow, area_sqmi)

dat_big <- dat_sub %>% filter(site_name == "West Brook NWIS") %>% select(site_name, datetime, flow, area_sqmi)
```


Check time zones
```{r}
unique(tz(dat_little$datetime))
unique(tz(dat_big$datetime))
```

Organize 15-min data
```{r}
dat_15min <- bind_rows(dat_little, dat_big) %>%
  mutate(site_name = factor(site_name, levels = c("West Brook Lower", "Mitchell Brook", "Jimmy Brook", "Obear Brook Lower", "West Brook Upper", "West Brook Reservoir", "Sanderson Brook", "Avery Brook", "West Whately Brook", "West Brook NWIS"))) %>%
  mutate(flow_cms = flow*0.02831683199881, area_sqkm = area_sqmi*2.58999) %>%
  mutate(yield = flow_cms * 900 * (1/(area_sqkm)) * (1/1000000) * 1000)
head(dat_15min)
```

Organize 1-hour data
```{r}
dat_1hr <- bind_rows(dat_little, dat_big) %>%
  mutate(site_name = factor(site_name, levels = c("West Brook Lower", "Mitchell Brook", "Jimmy Brook", "Obear Brook Lower", "West Brook Upper", "West Brook Reservoir", "Sanderson Brook", "Avery Brook", "West Whately Brook", "West Brook NWIS"))) %>%
  filter(!is.na(flow)) %>% 
  mutate(datetime = floor_date(datetime, unit = "hour")) %>%
  group_by(site_name, datetime) %>% 
  summarise(flow = mean(flow), area_sqmi = unique(area_sqmi)) %>%
  ungroup() %>%
  mutate(flow_cms = flow*0.02831683199881, area_sqkm = area_sqmi*2.58999) %>%
  mutate(yield = flow_cms * 3600 * (1/(area_sqkm)) * (1/1000000) * 1000)
head(dat_1hr)
```

Load daily data
```{r}
dat_1day <- read_csv("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/EcoDrought_FlowTempData_DailyWeekly.csv") %>%
  filter(site_name %in% c("West Brook Lower", "Mitchell Brook", "Jimmy Brook", "Obear Brook Lower", "West Brook Upper", "West Brook Reservoir", "Sanderson Brook", "Avery Brook", "West Whately Brook", "West Brook NWIS")) %>%
  mutate(site_name = factor(site_name, levels = c("West Brook Lower", "Mitchell Brook", "Jimmy Brook", "Obear Brook Lower", "West Brook Upper", "West Brook Reservoir", "Sanderson Brook", "Avery Brook", "West Whately Brook", "West Brook NWIS")))
head(dat_1day)
```

### View 15-min data

Plot 15 min time series data
```{r}
dat_15min %>% select(datetime, site_name, yield) %>% spread(key = site_name, value = yield) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "Yield (mm)") %>% dyOptions(colors = c(hcl.colors(9, "Zissou 1"), "black")) %>% dyHighlight() 
```

### View 1-hour data

Plot 1-hour time series data
```{r}
dat_1hr %>% select(datetime, site_name, yield) %>% spread(key = site_name, value = yield) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "Yield (mm)") %>% dyOptions(colors = c(hcl.colors(9, "Zissou 1"), "black")) %>% dyHighlight() 
```

### View 1-day data

Plot 1-day/daily time series data
```{r}
dat_1day %>% select(date, site_name, Yield_filled_mm) %>% spread(key = site_name, value = Yield_filled_mm) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "Yield (mm)") %>% dyOptions(colors = c(hcl.colors(9, "Zissou 1"), "black")) %>% dyHighlight() 
```


## Hourly data

set baseflow separation (for the Lyne-Hollick one-parameter digital recursive filter) and event delineation paramters (as in Wasko and Guo, 2022)

* *alp*: alpha filter parameter, higher values "lower" the estimated baseflow (thus making it difficult to delineate events)
* *numpass*: number of passes. Ladson *et al*. (2013) recommend 3 passes for daily data (default in baseflowB() function) and 9 passes for hourly data
* *thresh*: baseflow index threshold for event delineation, higher threshold values make it "easier" to delineate events

### Event pairing

Conduct event pairing using hydroEvents package to understand lag time between peak flows at big and little g's

#### Conduct event pairing

```{r}
# baseflow separation and event delineation parameters
alp <- 0.925
numpass <- 9
thresh <- 0.5

# sites
sites <- unique(dat_1hr$site_name)[1:9]

# empty list to store output
outlist <- list()

for (j in 1:length(sites)) {
  # grab big and little g data and combine into a single tibble
  littleg <- dat_1hr %>% filter(site_name == sites[j])
  bigg <- dat_1hr %>% filter(site_name == "West Brook NWIS")
  mytib <- bigg %>% select(datetime, yield) %>% rename(yield_big = yield) %>% left_join(littleg %>% select(datetime, yield) %>% rename(yield_little = yield))
  
  # baseflow separation
  mytib_bf <- mytib %>% 
  filter(!is.na(yield_big), !is.na(yield_little)) %>% 
  mutate(bf_big = baseflowB(yield_big, alpha = alp, passes = numpass)$bf, 
         bfi_big = baseflowB(yield_big, alpha = alp, passes = numpass)$bfi,
         bf_little = baseflowB(yield_little, alpha = alp, passes = numpass)$bf, 
         bfi_little = baseflowB(yield_little, alpha = alp, passes = numpass)$bfi)
  
  # delineate events
  events_little <- eventBaseflow(mytib_bf$yield_little, BFI_Th = thresh, bfi = mytib_bf$bfi_little)
  events_big <- eventBaseflow(mytib_bf$yield_big, BFI_Th = thresh, bfi = mytib_bf$bfi_big)
  
  # event matching
  mypairs <- pairEvents(events_little, events_big, lag = 5, type = 1)
  mypairs_com <- mypairs[complete.cases(mypairs),]
  
  # get matched event info
  matchpeaktib <- tibble(index_little = rep(NA, times = dim(mypairs_com)[1]), 
                         index_big = rep(NA, times = dim(mypairs_com)[1]),
                         datetime_little = rep(NA, times = dim(mypairs_com)[1]), 
                         datetime_big = rep(NA, times = dim(mypairs_com)[1]),
                         yield_little = rep(NA, times = dim(mypairs_com)[1]), 
                         yield_big = rep(NA, times = dim(mypairs_com)[1]))
  for (i in 1:dim(mypairs_com)[1]) {
    matchpeaktib$index_little[i] <- events_little$which.max[events_little$srt == mypairs_com$srt[i]]
    matchpeaktib$index_big[i] <- events_big$which.max[events_big$srt == mypairs_com$matched.srt[i]]
    matchpeaktib$datetime_little[i] <- mytib_bf$datetime[events_little$which.max[events_little$srt == mypairs_com$srt[i]]]
    matchpeaktib$datetime_big[i] <- mytib_bf$datetime[events_big$which.max[events_big$srt == mypairs_com$matched.srt[i]]]
    matchpeaktib$yield_little[i] <- events_little$max[events_little$srt == mypairs_com$srt[i]]
    matchpeaktib$yield_big[i] <- events_big$max[events_big$srt == mypairs_com$matched.srt[i]]
    }
  matchpeaktib <- matchpeaktib %>% mutate(datetime_little = as_datetime(datetime_little),
                                          datetime_big = as_datetime(datetime_big),
                                          timediff_hrs = as.numeric(difftime(datetime_big, datetime_little), units = "hours"),
                                          site_name = sites[j])
  
  # store output in list
  outlist[[j]] <- matchpeaktib
}
matchpeaktib <- do.call(rbind, outlist)
(matchpeaktib)
```


#### Plot output 

##### Distribution of lags

Constrain lag times to realistic values (>=0 and <= 24) as event pairing is not perfect, and view histograms by site
```{r}
matchpeaktib %>% filter(timediff_hrs >= -5 & timediff_hrs <= 10) %>% ggplot() + geom_histogram(aes(x = timediff_hrs)) + facet_wrap(~site_name)
```

View distributions summarized as boxplots. Sites are ordered from closest to Big G (bottom) to furthest (top). Interestingly, there is not a strong pattern of longer lag times for further sites. 
```{r}
# matchpeaktib %>% filter(timediff_hrs >= -10 & timediff_hrs <= 20) %>% ggplot() + geom_boxplot(aes(x = site_name, y = timediff_hrs)) + coord_flip()
matchpeaktib %>% filter(timediff_hrs >= -5 & timediff_hrs <= 10) %>% mutate(bigevcd = as.numeric(datetime_big)) %>% group_by(bigevcd, site_name) %>% summarize(timediff_hrs = min(timediff_hrs)) %>% ggplot() + geom_boxplot(aes(x = site_name, y = timediff_hrs)) + coord_flip()
```

Connect specific events across sites. 

* Is there consistency in lag time across sites? Generally, yes
* is there a pattern of longer lag times with increasing distance upstream? No
```{r}
matchpeaktib %>% filter(timediff_hrs >= -5 & timediff_hrs <= 10) %>% mutate(bigevcd = as.factor(as.numeric(datetime_big))) %>% group_by(bigevcd, site_name) %>% summarize(timediff_hrs = min(timediff_hrs)) %>% ggplot(aes(x = site_name, y = jitter(timediff_hrs), group = bigevcd, color = bigevcd)) + geom_line() + coord_flip() + theme(legend.position = "none")
```


##### Lag by yield

Does lag time depend on the magnitude of yield at big G? Under high flows when water is moving more quickly, we might expect the lag to be shorter (negative relationship).

View global relationship
```{r}
matchpeaktib %>% filter(timediff_hrs >= -5 & timediff_hrs <= 10) %>% ggplot(aes(x = yield_big, y = jitter(timediff_hrs))) + geom_point() + geom_smooth()
```

View by site
```{r fig.height=7, fig.width=7}
matchpeaktib %>% filter(timediff_hrs >= -5 & timediff_hrs <= 10) %>% ggplot(aes(x = yield_big, y = jitter(timediff_hrs))) + geom_point() + geom_smooth() + facet_wrap(~ site_name)
```

There appears to be some evidence for shorter lag times with increasing flow, but this relationship is only evident for very low flows. What is more apparent is the overall attenuation in variability in lag time as flows increase: at very low flows, lags are highly variable, but less variable (and intermediate in magnitude) under high flows. 


##### Lag by time

Does lag time change over time? Perhaps lag time is seasonal and changes with antecedant conditions. *Note that this is not the best way to get at the question of importance of antecedant conditions.*

View global relationship
```{r}
matchpeaktib %>% filter(timediff_hrs >= -5 & timediff_hrs <= 10) %>% mutate(doy = yday(datetime_little)) %>% ggplot(aes(x = doy, y = jitter(timediff_hrs))) + geom_point() + geom_smooth()
```

View by site
```{r fig.height=7, fig.width=7}
matchpeaktib %>% filter(timediff_hrs >= -5 & timediff_hrs <= 10) %>% mutate(doy = yday(datetime_little)) %>% ggplot(aes(x = doy, y = jitter(timediff_hrs))) + geom_point() + geom_smooth() + facet_wrap(~ site_name)
```

There does not appear to be any consistent pattern (within or among sites) in how lag times change with time of year


### Gg pseudo-analysis

#### Prepare data

Specify big and little g data
```{r}
dat_big <- dat_1hr %>% filter(site_name %in% c("West Brook NWIS"))
dat_little <- dat_1hr %>% filter(site_name %in% c("West Brook Lower", "Mitchell Brook", "Jimmy Brook", "Obear Brook Lower", "West Brook Upper", "West Brook Reservoir", "Sanderson Brook", "Avery Brook", "West Whately Brook"))
```

Conduct baseflow separation and event delineation on big g data
```{r}
# baseflow separation
dat_big <- dat_big %>% 
  filter(!is.na(yield)) %>% 
  mutate(bf = baseflowB(yield, alpha = 0.925, passes = 9)$bf, 
         bfi = baseflowB(yield, alpha = 0.925, passes = 9)$bfi)

# event delineation
events <- eventBaseflow(dat_big$yield, BFI_Th = 0.75, bfi = dat_big$bfi)
events <- events %>% mutate(len = end - srt + 1)

# define positions of non-events
srt <- c(1)
end <- c(events$srt[1]-1)
for (i in 2:(dim(events)[1])) {
  srt[i] <- events$end[i-1]+1
  end[i] <- events$srt[i]-1
}
nonevents <- data.frame(tibble(srt, end) %>% mutate(len = end - srt) %>% filter(len >= 0) %>% select(-len) %>% add_row(srt = events$end[dim(events)[1]]+1, end = dim(dat_big)[1]))

# create vectors of binary event/non-event and event IDs
isevent_vec <- rep(2, times = dim(dat_big)[1])
eventid_vec <- rep(NA, times = dim(dat_big)[1])
for (i in 1:dim(events)[1]) { 
  isevent_vec[c(events[i,1]:events[i,2])] <- 1 
  eventid_vec[c(events[i,1]:events[i,2])] <- i
}

# create vector of non-event IDs
noneventid_vec <- rep(NA, times = dim(dat_big)[1])
for (i in 1:dim(nonevents)[1]) { noneventid_vec[c(nonevents[i,1]:nonevents[i,2])] <- i }

# create vector of "agnostic events": combined hydro events and non-events
agnevents <- rbind(events %>% select(srt, end) %>% mutate(event = 1), nonevents %>% mutate(event = 0)) %>% arrange((srt))
agneventid_vec <- c()
for (i in 1:dim(agnevents)[1]){ agneventid_vec[c(agnevents[i,1]:agnevents[i,2])] <- i }

# add event/non-event vectors to Big G data
dat_big <- dat_big %>% 
  mutate(isevent = isevent_vec, 
         eventid = eventid_vec,
         noneventid = noneventid_vec,
         agneventid = agneventid_vec,
         big_event_yield = ifelse(isevent_vec == 1, yield, NA),
         big_nonevent_yield = ifelse(isevent_vec == 2, yield, NA),
         big_event_quick = big_event_yield - bf) %>%
  rename(big_yield = yield, big_bf = bf, big_bfi = bfi)
dat_big

# plot
dat_big %>% select(datetime, big_yield, big_bf, big_event_yield, big_nonevent_yield) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "Hourly yield (mm)") %>% dyOptions(fillGraph = TRUE, drawGrid = FALSE, axisLineWidth = 1.5)
```

A few things to note:

* Many delineated events are <24 hours in length
* Much of the natural diel variation in streamflow ("stream breathing" due to diel cycle of ET) ends up being delineated as individual events
  + When viewed at this time-scale, there also seems to be variation in terms of whether or not the "stream breathing" exists, which could be due to changes in how the data were processed and subsequently smoothed (or not)
* WMA interpolataion becomes an issue at the sub-daily time scale. I.e., the time scale of the data changes during observed (15-min) and interpolated (4-hour) periods


Combine big and little g data
```{r}
dat_wb2 <- dat_little %>% 
  filter(datetime >= min(dat_big$datetime) & datetime <= max(dat_big$datetime)) %>%
  left_join(dat_big %>% select(datetime, big_yield, big_bf, big_bfi, agneventid, isevent)) %>%
  group_by(site_name, agneventid) %>% 
  summarise(eventlen = n(),
            mindate = min(datetime),
            isevent = unique(isevent), 
            yield_little_cumul = sum(yield+0.01),
            yield_big_cumul = sum(big_yield+0.01),
            yield_little_cumul_log = log(yield_little_cumul),
            yield_big_cumul_log = log(yield_big_cumul),
            yield_little_mean_log = mean(log(yield+0.01)),
            yield_big_mean_log = mean(log(big_yield+0.01))) %>%
  ungroup() %>%
  filter(!is.na(isevent)) %>%
  mutate(site_name = factor(site_name, levels = c("West Brook Lower", "Mitchell Brook", "Jimmy Brook", "Obear Brook Lower", "West Brook Upper", "West Brook Reservoir", "Sanderson Brook", "Avery Brook", "West Whately Brook")),
         site_name_cd = as.numeric(site_name))
dat_wb2
```

#### Event-level distribution

Find ~long events/non-events
```{r}
agntib1 <- dat_big %>% group_by(agneventid) %>% summarize(eventlen = n(), isevent = unique(isevent), mindate = min(date(datetime)), maxdate = max(date(datetime))) %>% filter(isevent == 1) %>% arrange(desc(eventlen))
agntib2 <- dat_big %>% group_by(agneventid) %>% summarize(eventlen = n(), isevent = unique(isevent), mindate = min(date(datetime)), maxdate = max(date(datetime))) %>% filter(isevent == 2) %>% arrange(desc(eventlen))
agntib1
agntib2
```


```{r echo=FALSE}
myevents <- c(159, 330, 308, 18)
mytext <- agntib1 %>% filter(agneventid %in% myevents)

ddd1 <- dat_little %>% 
  filter(datetime >= min(dat_big$datetime) & datetime <= max(dat_big$datetime)) %>%
  left_join(dat_big %>% select(datetime, big_yield, big_bf, big_bfi, agneventid, isevent)) %>%
  filter(agneventid %in% myevents) %>%
  mutate(log_yield = log(yield + 0.001), log_big_yield = log(big_yield + 0.001)) %>%
  group_by(site_name, agneventid) %>%
  arrange(desc(log_yield), .by_group = TRUE) %>%
  mutate(exceedance = 100/length(log_yield)*1:length(log_yield)) %>%
  ungroup() 

ddd2 <- dat_little %>% 
  filter(datetime >= min(dat_big$datetime) & datetime <= max(dat_big$datetime)) %>%
  left_join(dat_big %>% select(datetime, big_yield, big_bf, big_bfi, agneventid, isevent)) %>%
  filter(agneventid %in% myevents, site_name == "West Brook Lower") %>%
  mutate(log_yield = log(yield + 0.001), log_big_yield = log(big_yield + 0.001)) %>%
  group_by(site_name, agneventid) %>%
  arrange(desc(log_big_yield), .by_group = TRUE) %>%
  mutate(exceedance = 100/length(log_big_yield)*1:length(log_big_yield)) %>%
  ungroup() 

p1 <- ggplot() +
  geom_line(data = ddd1, aes(x = exceedance, y = log_yield, group = site_name, color = site_name)) + 
  geom_line(data = ddd2, aes(x = exceedance, y = log_big_yield), color = "black") + 
  xlab("Exceedance probability") + ylab("ln(Yield, mm)") + labs(color = "Little g") + theme_bw() + 
  facet_wrap(~agneventid, nrow = 1) + ylim(-7,1.5) + theme(legend.position = "none")
x_range <- layer_scales(p1)$x$range$range
y_range <- layer_scales(p1)$y$range$range
p1 <- p1 + geom_text(data = mytext, 
              x = x_range[1] - 0.1 * (diff(x_range)),
              y = y_range[1] + 0.1 * (diff(y_range)),
              aes(label = paste(mindate, "\n", maxdate, "\nn = ", eventlen, " hrs", sep = "")),
              hjust = -0.3, vjust = 0.3, size = 3)
```

```{r echo=FALSE}
myevents <- c(152, 278, 71, 205)
mytext <- agntib2 %>% filter(agneventid %in% myevents)

ddd1 <- dat_little %>% 
  filter(datetime >= min(dat_big$datetime) & datetime <= max(dat_big$datetime)) %>%
  left_join(dat_big %>% select(datetime, big_yield, big_bf, big_bfi, agneventid, isevent)) %>%
  filter(agneventid %in% myevents) %>%
  mutate(log_yield = log(yield + 0.001), log_big_yield = log(big_yield + 0.001)) %>%
  group_by(site_name, agneventid) %>%
  arrange(desc(log_yield), .by_group = TRUE) %>%
  mutate(exceedance = 100/length(log_yield)*1:length(log_yield)) %>%
  ungroup() 

ddd2 <- dat_little %>% 
  filter(datetime >= min(dat_big$datetime) & datetime <= max(dat_big$datetime)) %>%
  left_join(dat_big %>% select(datetime, big_yield, big_bf, big_bfi, agneventid, isevent)) %>%
  filter(agneventid %in% myevents, site_name == "West Brook Lower") %>%
  mutate(log_yield = log(yield + 0.001), log_big_yield = log(big_yield + 0.001)) %>%
  group_by(site_name, agneventid) %>%
  arrange(desc(log_big_yield), .by_group = TRUE) %>%
  mutate(exceedance = 100/length(log_big_yield)*1:length(log_big_yield)) %>%
  ungroup() 

p2 <- ggplot() +
  geom_line(data = ddd1, aes(x = exceedance, y = log_yield, group = site_name, color = site_name)) + 
  geom_line(data = ddd2, aes(x = exceedance, y = log_big_yield), color = "black") + 
  xlab("Exceedance probability") + ylab("ln(Yield, mm)") + labs(color = "Little g") + theme_bw() + 
  facet_wrap(~agneventid, nrow = 1) + ylim(-7,1.5) + theme(legend.position = "none")
x_range <- layer_scales(p2)$x$range$range
y_range <- layer_scales(p2)$y$range$range
p2 <- p2 + geom_text(data = mytext, 
              x = x_range[1] - 0.1 * (diff(x_range)),
              y = y_range[1] + 0.1 * (diff(y_range)),
              aes(label = paste(mindate, "\n", maxdate, "\nn = ", eventlen, " hrs", sep = "")),
              hjust = -0.3, vjust = 0.3, size = 3)
```
Plot exceedance curves for selected events (top row) and baseflow periods (bottom row)
```{r fig.width=9, fig.height=5}
ggarrange(p1, p2, nrow = 2)
```


#### Plot output

gG relationship with data summarized as cumulative yield per event/non-event:
```{r, fig.height=4, fig.width=9}
#| fig-cap: "Effect of (log) cumulative yield at Big G on (log) cumulative yield at little g during baseflow and event periods."
dat_wb2 %>% 
  mutate(isevent = dplyr::recode(isevent, "1" = "Event", "2" = "Baseflow")) %>% 
  ggplot(aes(x = yield_big_cumul_log, y = yield_little_cumul_log, group = site_name, color = site_name)) + 
  geom_point(alpha = 0.25) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + 
  geom_smooth(method = "lm", se = F) + facet_wrap(~isevent)
```

Facet by site
```{r fig.height=7, fig.width=7}
#| fig-cap: "Effect of (log) cumulative yield at Big G on (log) cumulative yield at little g during baseflow and event periods, faceted by site."
dat_wb2 %>% 
  mutate(isevent = dplyr::recode(isevent, "1" = "Event", "2" = "Baseflow")) %>% 
  ggplot(aes(x = yield_big_cumul_log, y = yield_little_cumul_log, group = site_name, color = site_name)) + 
  geom_point(alpha = 0.25) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + 
  geom_smooth(method = "lm", se = F) + facet_wrap(~site_name) +
  theme(legend.position = "none")
```

Gg relationship with data summarized as mean yield per event/non-event
```{r, fig.height=4, fig.width=9}
#| fig-cap: "Effect of (log) mean yield at Big G on (log) mean yield at little g during baseflow and event periods."
dat_wb2 %>% 
  mutate(isevent = dplyr::recode(isevent, "1" = "Event", "2" = "Baseflow")) %>% 
  ggplot(aes(x = yield_big_mean_log, y = yield_little_mean_log, group = site_name, color = site_name)) + 
  geom_point(alpha = 0.25) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + 
  geom_smooth(method = "lm", se = F) + facet_wrap(~isevent)
```

Facet by site
```{r fig.height=7, fig.width=7}
#| fig-cap: "Effect of (log) mean yield at Big G on (log) mean yield at little g during baseflow and event periods, faceted by site."
dat_wb2 %>% 
  mutate(isevent = dplyr::recode(isevent, "1" = "Event", "2" = "Baseflow")) %>% 
  ggplot(aes(x = yield_big_mean_log, y = yield_little_mean_log, group = site_name, color = site_name)) + 
  geom_point(alpha = 0.25) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + 
  geom_smooth(method = "lm", se = F) + facet_wrap(~site_name) +
  theme(legend.position = "none")
```


## Daily data

### Event pairing

#### Conduct event pairing
```{r}
# baseflow separation and event delineation parameters
alp <- 0.925
numpass <- 3
thresh <- 0.5

# sites
sites <- unique(dat_1day$site_name)[1:9]

# empty list to store output
outlist <- list()

for (j in 1:length(sites)) {
  # grab big and little g data and combine into a single tibble
  littleg <- dat_1day %>% filter(site_name == sites[j])
  bigg <- dat_1day %>% filter(site_name == "West Brook NWIS")
  mytib <- bigg %>% select(date, Yield_filled_mm) %>% rename(yield_big = Yield_filled_mm) %>% left_join(littleg %>% select(date, Yield_filled_mm) %>% rename(yield_little = Yield_filled_mm))
  
  # baseflow separation
  mytib_bf <- mytib %>% 
  filter(!is.na(yield_big), !is.na(yield_little)) %>% 
  mutate(bf_big = baseflowB(yield_big, alpha = alp, passes = numpass)$bf, 
         bfi_big = baseflowB(yield_big, alpha = alp, passes = numpass)$bfi,
         bf_little = baseflowB(yield_little, alpha = alp, passes = numpass)$bf, 
         bfi_little = baseflowB(yield_little, alpha = alp, passes = numpass)$bfi)
  
  # delineate events
  events_little <- eventBaseflow(mytib_bf$yield_little, BFI_Th = thresh, bfi = mytib_bf$bfi_little)
  events_big <- eventBaseflow(mytib_bf$yield_big, BFI_Th = thresh, bfi = mytib_bf$bfi_big)
  
  # event matching
  mypairs <- pairEvents(events_little, events_big, lag = 5, type = 1)
  mypairs_com <- mypairs[complete.cases(mypairs),]
  
  # get matched event info
  matchpeaktib <- tibble(datetime_little = rep(NA, times = dim(mypairs_com)[1]), datetime_big = rep(NA, times = dim(mypairs_com)[1]),
                         yield_little = rep(NA, times = dim(mypairs_com)[1]), yield_big = rep(NA, times = dim(mypairs_com)[1]))
  for (i in 1:dim(mypairs_com)[1]) {
    matchpeaktib$datetime_little[i] <- mytib_bf$date[events_little$which.max[events_little$srt == mypairs_com$srt[i]]]
    matchpeaktib$datetime_big[i] <- mytib_bf$date[events_big$which.max[events_big$srt == mypairs_com$matched.srt[i]]]
    matchpeaktib$yield_little[i] <- events_little$max[events_little$srt == mypairs_com$srt[i]]
    matchpeaktib$yield_big[i] <- events_big$max[events_big$srt == mypairs_com$matched.srt[i]]
    }
  matchpeaktib <- matchpeaktib %>% mutate(datetime_little = as_date(datetime_little),
                                          datetime_big = as_date(datetime_big),
                                          timediff_dys = as.numeric(difftime(datetime_big, datetime_little), units = "days"),
                                          site_name = sites[j])
  
  # store output in list
  outlist[[j]] <- matchpeaktib
}
matchpeaktib <- do.call(rbind, outlist)
(matchpeaktib)
```

#### Plot output

##### Distribution of lags

Constrain lag times to realistic values (>=0 and <= 24) as event pairing is not perfect, and view histograms by site
```{r}
matchpeaktib %>% filter(timediff_dys >= 0 & timediff_dys <= 1) %>% ggplot() + geom_histogram(aes(x = timediff_dys)) + facet_wrap(~site_name)
```

View distributions summarized as means. Sites are ordered from closest to Big G (bottom) to furthest (top). There is general pattern of longer lag times for further sites. 
```{r}
matchpeaktib %>% filter(timediff_dys >= 0 & timediff_dys <= 1) %>% group_by(site_name) %>% summarize(diffmean = mean(timediff_dys), diffsd = sd(timediff_dys)) %>% ggplot() + 
  geom_point(aes(x = site_name, y = diffmean)) + 
  #geom_errorbar(aes(x = site_name, ymin = diffmean - diffsd, ymax = diffmean + diffsd)) + 
  coord_flip()
```

##### Lag by yield

Does lag time depend on the magnitude of yield at big G? Under high flows when water is moving more quickly, we might expect the lag to be shorter (negative relationship).

View global relationship
```{r}
matchpeaktib %>% filter(timediff_dys >= 0 & timediff_dys <= 1) %>% ggplot(aes(x = yield_big, y = jitter(timediff_dys))) + geom_point() + geom_smooth()
```

View by site
```{r fig.height=7, fig.width=7}
matchpeaktib %>% filter(timediff_dys >= 0 & timediff_dys <= 1)  %>% ggplot(aes(x = yield_big, y = jitter(timediff_dys))) + geom_point() + geom_smooth() + facet_wrap(~ site_name)
```

As with the hourly data, there appears to be some evidence for shorter lag times with increasing flow, but this relationship is only evident for very low flows. Although we note that 1-day lags are very rare (16% of all observations)


##### Lag by time

Does lag time change over time? Perhaps lag time is seasonal and changes with antecedant conditions. *Note that this is not the best way to get at the question of importance of antecedant conditions.*

View global relationship
```{r}
matchpeaktib %>% filter(timediff_dys >= 0 & timediff_dys <= 1) %>% mutate(doy = yday(datetime_little)) %>% ggplot(aes(x = doy, y = jitter(timediff_dys))) + geom_point() + geom_smooth()
```

View by site
```{r fig.height=7, fig.width=7}
matchpeaktib %>% filter(timediff_dys >= 0 & timediff_dys <= 1) %>% mutate(doy = yday(datetime_little)) %>% ggplot(aes(x = doy, y = (timediff_dys))) + geom_point() + geom_smooth() + facet_wrap(~ site_name)
```



```{r eval=FALSE, include=FALSE}
# plot matched peaks
test <- matchpeaktib %>% mutate(paircode = c(1:dim(matchpeaktib)[1])) %>% select(paircode, datetime_little, yield_little) %>% rename(datetime = datetime_little, yield = yield_little) %>% bind_rows(matchpeaktib %>% mutate(paircode = c(1:dim(matchpeaktib)[1])) %>% select(paircode, datetime_big, yield_big) %>% rename(datetime = datetime_big, yield = yield_big)) %>% arrange(paircode) 
# test$datetime[52] <- test$datetime[52] + seconds(1)
# test <- test %>% spread(key = paircode, value = yield)
# test$datetime[51] <- test$datetime[51] - seconds(1)

mytib_bf %>% select(datetime, yield_big, yield_little) %>% left_join(test) %>% dygraph() %>% dyRangeSelector(retainDateWindow = TRUE) %>% dyAxis("y", label = "Yield (mm)") %>% dyOptions(colors = c("darkgreen", "purple", rep("black", times = dim(test)[2]-1)), drawPoints = TRUE, pointSize = 2, connectSeparatedPoints = TRUE) %>% dySeries(name = "yield_big", drawPoints = FALSE) %>% dySeries(name = "yield_little", drawPoints = FALSE) #%>% dyLegend(show = "never")
```


### Gg pseudo-analysis

#### Prepare data

Specify big and little g data
```{r}
dat_big <- dat_1day %>% filter(site_name %in% c("West Brook NWIS")) %>% rename(yield = Yield_filled_mm)
dat_little <- dat_1day %>% filter(site_name %in% c("West Brook Lower", "Mitchell Brook", "Jimmy Brook", "Obear Brook Lower", "West Brook Upper", "West Brook Reservoir", "Sanderson Brook", "Avery Brook", "West Whately Brook")) %>% rename(yield = Yield_filled_mm)
```

Conduct baseflow separation and event delineation on big g data
```{r}
# baseflow separation
dat_big <- dat_big %>% 
  filter(!is.na(yield)) %>% 
  mutate(bf = baseflowB(yield, alpha = 0.925, passes = 3)$bf, 
         bfi = baseflowB(yield, alpha = 0.925, passes = 3)$bfi)

# event delineation
events <- eventBaseflow(dat_big$yield, BFI_Th = 0.75, bfi = dat_big$bfi)
events <- events %>% mutate(len = end - srt + 1)

# define positions of non-events
srt <- c(1)
end <- c(events$srt[1]-1)
for (i in 2:(dim(events)[1])) {
  srt[i] <- events$end[i-1]+1
  end[i] <- events$srt[i]-1
}
nonevents <- data.frame(tibble(srt, end) %>% mutate(len = end - srt) %>% filter(len >= 0) %>% select(-len) %>% add_row(srt = events$end[dim(events)[1]]+1, end = dim(dat_big)[1]))

# create vectors of binary event/non-event and event IDs
isevent_vec <- rep(2, times = dim(dat_big)[1])
eventid_vec <- rep(NA, times = dim(dat_big)[1])
for (i in 1:dim(events)[1]) { 
  isevent_vec[c(events[i,1]:events[i,2])] <- 1 
  eventid_vec[c(events[i,1]:events[i,2])] <- i
}

# create vector of non-event IDs
noneventid_vec <- rep(NA, times = dim(dat_big)[1])
for (i in 1:dim(nonevents)[1]) { noneventid_vec[c(nonevents[i,1]:nonevents[i,2])] <- i }

# create vector of "agnostic events": combined hydro events and non-events
agnevents <- rbind(events %>% select(srt, end) %>% mutate(event = 1), nonevents %>% mutate(event = 0)) %>% arrange((srt))
agneventid_vec <- c()
for (i in 1:dim(agnevents)[1]){ agneventid_vec[c(agnevents[i,1]:agnevents[i,2])] <- i }

# add event/non-event vectors to Big G data
dat_big <- dat_big %>% 
  mutate(isevent = isevent_vec, 
         eventid = eventid_vec,
         noneventid = noneventid_vec,
         agneventid = agneventid_vec,
         big_event_yield = ifelse(isevent_vec == 1, yield, NA),
         big_nonevent_yield = ifelse(isevent_vec == 2, yield, NA),
         big_event_quick = big_event_yield - bf) %>%
  rename(big_yield = yield, big_bf = bf, big_bfi = bfi)
dat_big

# plot
dat_big %>% select(date, big_yield, big_bf, big_event_yield, big_nonevent_yield) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "Hourly yield (mm)") %>% dyOptions(fillGraph = TRUE, drawGrid = FALSE, axisLineWidth = 1.5)
```


Combine big and little g data
```{r}
dat_wb2 <- dat_little %>% 
  filter(date >= min(dat_big$date) & date <= max(dat_big$date)) %>%
  left_join(dat_big %>% select(date, big_yield, big_bf, big_bfi, agneventid, isevent)) %>%
  group_by(site_name, agneventid) %>% 
  summarise(eventlen = n(),
            mindate = min(date),
            isevent = unique(isevent), 
            yield_little_cumul = sum(yield+0.01),
            yield_big_cumul = sum(big_yield+0.01),
            yield_little_cumul_log = log(yield_little_cumul),
            yield_big_cumul_log = log(yield_big_cumul),
            yield_little_mean_log = mean(log(yield+0.01)),
            yield_big_mean_log = mean(log(big_yield+0.01))) %>%
  ungroup() %>%
  filter(!is.na(isevent)) %>%
  mutate(site_name = factor(site_name, levels = c("West Brook Lower", "Mitchell Brook", "Jimmy Brook", "Obear Brook Lower", "West Brook Upper", "West Brook Reservoir", "Sanderson Brook", "Avery Brook", "West Whately Brook")),
         site_name_cd = as.numeric(site_name))
dat_wb2
```


#### Plot output

gG relationship with data summarized as cumulative yield per event/non-event:
```{r, fig.height=4, fig.width=9}
#| fig-cap: "Effect of (log) cumulative yield at Big G on (log) cumulative yield at little g during baseflow and event periods."
dat_wb2 %>% 
  mutate(isevent = dplyr::recode(isevent, "1" = "Event", "2" = "Baseflow")) %>% 
  ggplot(aes(x = yield_big_cumul_log, y = yield_little_cumul_log, group = site_name, color = site_name)) + 
  geom_point(alpha = 0.25) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + 
  geom_smooth(method = "lm", se = F) + facet_wrap(~isevent)
```

Facet by site
```{r fig.height=7, fig.width=7}
#| fig-cap: "Effect of (log) cumulative yield at Big G on (log) cumulative yield at little g during baseflow and event periods, faceted by site."
dat_wb2 %>% 
  mutate(isevent = dplyr::recode(isevent, "1" = "Event", "2" = "Baseflow")) %>% 
  ggplot(aes(x = yield_big_cumul_log, y = yield_little_cumul_log, group = site_name, color = site_name)) + 
  geom_point(alpha = 0.25) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + 
  geom_smooth(method = "lm", se = F) + facet_wrap(~site_name) +
  theme(legend.position = "none")
```

Gg relationship with data summarized as mean yield per event/non-event
```{r, fig.height=4, fig.width=9}
#| fig-cap: "Effect of (log) mean yield at Big G on (log) mean yield at little g during baseflow and event periods."
dat_wb2 %>% 
  mutate(isevent = dplyr::recode(isevent, "1" = "Event", "2" = "Baseflow")) %>% 
  ggplot(aes(x = yield_big_mean_log, y = yield_little_mean_log, group = site_name, color = site_name)) + 
  geom_point(alpha = 0.25) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + 
  geom_smooth(method = "lm", se = F) + facet_wrap(~isevent)
```

Facet by site
```{r fig.height=7, fig.width=7}
#| fig-cap: "Effect of (log) mean yield at Big G on (log) mean yield at little g during baseflow and event periods, faceted by site."
dat_wb2 %>% 
  mutate(isevent = dplyr::recode(isevent, "1" = "Event", "2" = "Baseflow")) %>% 
  ggplot(aes(x = yield_big_mean_log, y = yield_little_mean_log, group = site_name, color = site_name)) + 
  geom_point(alpha = 0.25) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + 
  geom_smooth(method = "lm", se = F) + facet_wrap(~site_name) +
  theme(legend.position = "none")
```

## Summary

* Event delineation at sub-daily (i.e., hourly) time scale is strongly affected by data processing steps (e.g., smoothing and interpolation by WMA) and landscape processes that are not the focus of this study (i.e., diel fluctuations in flow due to ET)
  + In some cases, changes in how WMA treats diel fluctuations may introduce further inconsistencies into downstream analyses
* At the hourly timescale, temporal mismatches between big and little g time series data due to routing time are evident, typically in the 4-8 hour range for the West Brook.
  + There does not appear to be any predictable relationship between lag time and flow magnitude or time of year.
  + Because many of the event/non-event periods are very short, these mismatches may have a large effect on how reasonable it is to apply Big G events to little g data, and whether Big G events/non-events adequately encompass similar flow conditions at little g sites. 
  + Temporal mismatches are less prevalent and more predictable for the daily data. 
* Inferences regarding the g~G relationship derived from hourly data generally do not match those derived from daily data.
  + the g~G relationship is more strongly affected by assumptions of temporal alignment for hourly data relative to daily data.


## Dynamic time warping

Explore the use of dynamic time warping (Giorgino 2009) to align hourly time series data.

### Select data

Trim to restricted period b/c DTW cannot handle very large datasets
```{r}
littleg <- dat_1hr %>% filter(site_name == "Avery Brook", datetime >= as_datetime("2020-03-01 00:00:00") & datetime <= as_datetime("2020-06-01 00:00:00"))
bigg <- dat_1hr %>% filter(site_name == "West Brook NWIS", datetime >= as_datetime("2020-03-01 00:00:00") & datetime <= as_datetime("2020-06-01 00:00:00"))

mytib <- bigg %>% select(datetime, yield) %>% rename(yield_big = yield) %>% left_join(littleg %>% select(datetime, yield) %>% rename(yield_little = yield))
mytib %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "Yield (mm)")
```


### Align data
```{r}
align1hr <- dtw(x = unlist(littleg %>% select(yield)), y = unlist(bigg %>% select(yield)), step = asymmetric, keep = TRUE)
str(align1hr)
```

View index alignment
```{r}
plot(align1hr, type = "threeway")
```

Show aligned values
```{r fig.height=5, fig.width=15}
plot(align1hr, type = "twoway", offset = - 1)
```

View aligned timeseries using dyGraphs. Clearly, this is not a great approach as it matches multiple query data points to the same reference index, i.e., the result is multiple little g flow readings at a single time point. As seen in the plots above, it also does not align the series correctly. 
```{r}
aligneddata <- tibble(datetime = bigg$datetime[align1hr$index2], query = littleg$yield[align1hr$index1], reference = bigg$yield[align1hr$index2])
aligneddata %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "Yield (mm)")
```



