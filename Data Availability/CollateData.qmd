---
title: "Collate Data"
format: html
editor: visual
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, cache.lazy = FALSE)
```

Purpose: Collate EcoDrought streamflow and temperature data, povided by EcoD PIs/partners and NWIS

Notes: 


* need to address missing data issue, in the Flathead and elsewhere...interpolate at hourly timescale? Or do not interpolate at all...


```{r, include = FALSE}
library(tidyverse)
library(mapview)
library(sf)
library(zoo)
library(dataRetrieval)
library(nhdplusTools)
library(ggpubr)
library(fasstr)
library(dygraphs)
library(ggforce)
library(smwrBase)
library(shiny)
library(rmarkdown)
library(shiny)
library(htmlwidgets)
library(knitr)
```

## Gather site information

```{r eval=FALSE}
# West Brook
siteinfo_wb <- read_csv("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/Raw data/Mass/MA_site_info.csv") %>%
  mutate(Station_No = factor(Station_No), Site_Name = factor(Site_Name)) %>%
  rename(site_id = Site_ID, station_no = Station_No, site_name = Site_Name, lat = Latitude_dec_deg, long = Longitude_dec_deg, elev_ft = Elevation_ft, area_sqmi = Drainage_Area_sqmi) %>%
  mutate(designation = "little", basin = "West Brook", region = "Mass") #%>% select(-c(elev_ft, area_sqmi)) 

# Shenandoah
siteinfo_shen <- read_csv("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/Raw data/Virg/VA_site_info.csv") %>%
  mutate(Station_No = factor(Station_No), Site_Name = factor(Site_Name)) %>%
  rename(site_id = Site_ID, station_no = Station_No, site_name = Site_Name, lat = Latitude_dec_deg, long = Longitude_dec_deg, elev_ft = Elevation_ft, area_sqmi = Drainage_Area_sqmi) %>%
  mutate(designation = ifelse(str_detect(site_id, "10FL"), "big", "little"), 
         basin = str_sub(site_name, 1, str_length(site_name)-3), region = "Shen") #%>% select(-c(elev_ft, area_sqmi))

# Flathead/Muhlfeld
siteinfo_flat <- read_csv("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/Raw data/Flathead/Flathead_SiteInfo_UpdateOct25.csv") %>% 
  select(basin, site_name, site_id, region, designation, lat, long) %>%
  rename(subbasin = basin) %>%
  mutate(basin = "Flathead", region = "Flat") %>%
  select(site_id, site_name, lat, long, designation, basin, subbasin, region) %>%
  filter(designation == "little")
  
# GYA/Al-Chokhachy
siteinfo_gya <- read_csv("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/Raw data/Al-Chokhachy/Al-Chokhachy_sites.csv") %>%
  mutate(region = ifelse(basin == "Snake River", "Snake", "Shields"), 
         designation = "little") %>%
  select(site_id, site_name, latitude, longitude, designation, basin, region) %>% 
  rename(lat = latitude, long = longitude)
  

# NWIS Medium/Big/Super G
sites <- c("01169900", # South River at Conway, Massachusetts
           "13011500", # Pacific Creek, Snake River, Wyoming
           "06195600", # Shields River at Livingston, Montana
           "12355500", # North Fork Flathead River, Montana
           "10396000", # Donner Blitzen River near Frenchglen, Oregon
           # Medium G
           "12355347", # Big Creek (Flathead)
           "12355342", # Hallowat Creek (Flathead)
           "06192980", # Shields Rivera above Smith Creek (GYA)
           "06192900", # Dugout Creek Mouth (GYA)
           "13012475", # South Fork Spread Creek (GYA)
           "13012465", # Leidy Creek, lower (GYA)
           "01171100", # West Brook (Mass)
           "01171000",  # Avery Brook (Mass)
           "424551118503200", # Fish Creek at DB confluence (Oreg)
           "424547118503500", # DB above Fish Creek (Oreg)
           "424325118495900", # DB near Burnt Car Spring (Oreg)
           "424003118453700", # Little Blitzen River (Oreg)
           "423830118453200", # Indian Creek (Oreg)
           "423815118453900" # DB above Indian Creek (Oreg)
           )
siteinfo_nwis <- tibble(readNWISsite(sites)[,c(2:3,7,8,20,30)]) # get site info
names(siteinfo_nwis) <- c("station_no", "site_name", "lat", "long", "elev_ft", "area_sqmi") # rename columns
siteinfo_nwis <- siteinfo_nwis %>% mutate(site_name = c("South River Conway NWIS", 
                                                        "Avery Brook NWIS", 
                                                        "West Brook NWIS", 
                                                        "Dugout Creek NWIS", 
                                                        "Shields River ab Smith NWIS", 
                                                        "Shields River nr Livingston NWIS", 
                                                        "Donner Blitzen River nr Frenchglen NWIS", 
                                                        "Hallowat Creek NWIS", 
                                                        "Big Creek NWIS", 
                                                        "North Fork Flathead River NWIS", 
                                                        "Pacific Creek at Moran NWIS", 
                                                        "Leidy Creek Mouth NWIS", 
                                                        "SF Spread Creek Lower NWIS",
                                                        "Donner Blitzen ab Indian NWIS",
                                                        "Indian Creek NWIS",
                                                        "Little Blizten River NWIS",
                                                        "Donner Blitzen nr Burnt Car NWIS",
                                                        "Donner Blitzen ab Fish NWIS",
                                                        "Fish Creek NWIS"),
                                          site_id = c("SRC", "AVB", "WBR", "DUG", "SRS", "SRL", "DBF", "HAL", "BIG", "NFF", "PCM", "LEI", "SFS", "DBI", "IND", "LBL", "DBB", "DBA", "FSH"),
                                          designation = c("big", "medium", "medium", "medium", "medium", "big", "big", "medium", "medium", "big", "big", "medium", "medium", "medium", "medium", "medium", "medium", "medium", "medium"),
                                          basin = c("West Brook", "West Brook", "West Brook", "Shields River", "Shields River", "Shields River", "Donner Blitzen", "Flathead", "Flathead", "Flathead", "Snake River", "Snake River", "Snake River", "Donner Blitzen", "Donner Blitzen", "Donner Blitzen", "Donner Blitzen", "Donner Blitzen", "Donner Blitzen"),
                                          region = c("Mass", "Mass", "Mass", "Shields", "Shields", "Shields", "Oreg", "Flat", "Flat", "Flat", "Snake", "Snake", "Snake","Oreg", "Oreg", "Oreg", "Oreg", "Oreg", "Oreg")) %>% 
  select(site_id, site_name, lat, long, station_no, designation, basin, region, elev_ft, area_sqmi)
#mapview(st_as_sf(siteinfo_nwis, coords = c("long", "lat"), crs = 4326))


# bind together, fill in ragged subbasin
siteinfo <- bind_rows(siteinfo_wb, siteinfo_shen, siteinfo_flat, siteinfo_gya, siteinfo_nwis)
siteinfo$subbasin[siteinfo$site_name == "Hallowat Creek NWIS"] <- "Big Creek"
siteinfo$subbasin[siteinfo$site_name == "Big Creek NWIS"] <- "Big Creek"
siteinfo <- siteinfo %>% mutate(subbasin = ifelse(is.na(subbasin), basin, subbasin))


# fix Shields River Valley Ranch site locations
siteinfo$lat[siteinfo$site_id == "SH07"] <- siteinfo$lat[siteinfo$site_id == "SRS"]
siteinfo$long[siteinfo$site_id == "SH07"] <- siteinfo$long[siteinfo$site_id == "SRS"]

# add data source 
siteinfo$source <- ifelse(grepl("NWIS", siteinfo$site_name), "NWIS", "ECOD")

# add elevation and area variables (from watershed delineation)
areafiles <- list.files("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/EcoDrought-Analysis/Watershed Delineation/Area and Elevation")
arealist <- list()
for (i in 1:length(areafiles)) { arealist[[i]] <- read_csv(paste("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/EcoDrought-Analysis/Watershed Delineation/Area and Elevation/", areafiles[i], sep = ""))}
areaelev <- do.call(rbind, arealist)
# how well do provided and delineation area/elevation match?
# siteinfo %>% left_join(areaelev, by = "site_id") %>% ggplot() + geom_point(aes(x = area_sqmi.x, y = area_sqmi.y)) + geom_abline(intercept = 0, slope = 1) + facet_wrap(~basin, scales = "free")
# test %>% ggplot() + geom_point(aes(x = elev_ft.x, y = elev_ft.y)) + geom_abline(intercept = 0, slope = 1) + facet_wrap(~basin, scales = "free")
# add delineated variables
siteinfo <- siteinfo %>% select(-c(area_sqmi, elev_ft)) %>% left_join(areaelev)
# fix NF Flathead (no dem from Canada)
siteinfo$area_sqmi[siteinfo$site_id == "NFF"] <- 1556
```

Write and re-load site information

```{r eval=FALSE}
write_csv(siteinfo, "C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/EcoDrought_SiteInformation.csv")
```

```{r}
siteinfo <- read_csv("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/EcoDrought_SiteInformation.csv")
head(siteinfo)
```


View unique basin names

```{r}
unique(siteinfo$basin)
```

View unique site names

```{r}
unique(siteinfo$site_name)
```

Map sites

```{r}
# convert to spatial object and view on map
#| fig-cap: "Map of EcoDrought project locations"
siteinfo_sp <- st_as_sf(siteinfo, coords = c("long", "lat"), crs = 4326)
mapview(siteinfo_sp, zcol = "designation")
```

## EcoD data

### Load EcoD data

Load EcoDrought flow and temperature data and convert all date/times to UTC
```{r}
# West Brook
dat_wb <- read_csv("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/Raw data/Mass/EcoDrought Continuous_MA.csv") %>%
  mutate(Station_No = factor(Station_No), Site_Name = factor(Site_Name)) %>%
  rename(station_no = Station_No, site_name = Site_Name, datetime = DateTime_EST, height = GageHeight_Hobo_ft, flow = Discharge_Hobo_cfs, tempf = WaterTemperature_HOBO_DegF) %>%
  mutate(tempc = (tempf - 32)*(5/9)) %>% select(station_no, site_name, datetime, height, flow, tempc) %>%
  left_join(siteinfo %>% select(-station_no) %>% filter(source == "ECOD")) %>%
  mutate(DischargeReliability = as.factor(1), TempReliability = as.factor(1))
# set tz to local and convert to UTC
tz(dat_wb$datetime) <- "EST"
dat_wb$datetime <- with_tz(dat_wb$datetime, "UTC")
head(dat_wb)


# Shenandoah
dat_shen <- read_csv("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/Raw data/Virg/EcoDrought_Continuous_VA.csv") %>%
  mutate(Station_No = factor(Station_No), Site_ID = factor(Site_ID), Discharge_Hobo_cfs = as.numeric(Discharge_Hobo_cfs)) %>%
  rename(station_no = Station_No, site_id = Site_ID, datetime = DateTime_EST, height = GageHeight_Hobo_ft, flow = Discharge_Hobo_cfs, tempf = WaterTemperature_HOBO_DegF) %>%
  mutate(tempc = (tempf - 32)*(5/9)) %>% select(station_no, site_id, datetime, height, flow, tempc) %>%
  left_join(siteinfo %>% filter(source == "ECOD"))
# set tz to local and convert to UTC
tz(dat_shen$datetime) <- "EST"
dat_shen$datetime <- with_tz(dat_shen$datetime, "UTC")

# pull in Big G data separately (UVA long-term gage sites)
dat_shen_uva_q <- read_csv("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/Raw data/Virg/Shen_BigG_Discharge_hourly_UVA.csv") %>%
  rename(flow = cfs)
dat_shen_uva_t <- read_csv("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/Raw data/Virg/Shen_BigG_TempEtc_hourly_UVA.csv") %>%
  select(site_id, datetime, tempc_mean) %>% rename(tempc = tempc_mean)
dat_shen_uva <- dat_shen_uva_q %>% left_join(dat_shen_uva_t) %>% left_join(siteinfo %>% filter(source == "ECOD"))
# set tz to local and convert to UTC
tz(dat_shen_uva$datetime) <- "EST"
dat_shen_uva$datetime <- with_tz(dat_shen_uva$datetime, "UTC")

# bind usgs and uva data
dat_shen <- bind_rows(dat_shen, dat_shen_uva) %>% mutate(DischargeReliability = as.factor(1), TempReliability = as.factor(1))
head(dat_shen)


# Flathead/Muhlfeld
flatfiles <- list.files("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/Raw data/Flathead/Export3/EMA/Continuous Data")
flatlist <- list()
for (i in 1:length(flatfiles)) { 
  flatlist[[i]] <- read_csv(paste("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/Raw data/Flathead/Export3/EMA/Continuous Data/", flatfiles[i], sep = "")) %>%
    mutate(#DateTime = mdy_hm(DateTime, tz = "MST"),
           site_name = gsub("EMA.csv", "", flatfiles[i])) %>% 
    select(ISO.8601.UTC, GageHeightFT, DischargeCFS, TemperatureF, site_name, DischargeReliability_JB, GageReliability, TempReliability_JB) %>% rename(DischargeReliability = DischargeReliability_JB, TempReliability = TempReliability_JB)
}
dat_flat <- bind_rows(flatlist) %>%
  filter(!is.na(ISO.8601.UTC)) %>% 
  rename(datetime = ISO.8601.UTC, height = GageHeightFT, flow = DischargeCFS, tempf = TemperatureF) %>%
  mutate(tempc = (tempf - 32) * (5/9), datetime = floor_date(datetime, unit = "minute")) %>% 
  select(-tempf) %>%
  group_by(site_name, datetime) %>% 
  summarise(height = mean(height, na.rm = TRUE), flow = mean(flow, na.rm = TRUE), tempc = mean(tempc, na.rm = TRUE),
            DischargeReliability = unique(DischargeReliability), 
            GageReliability = unique(GageReliability), 
            TempReliability = unique(TempReliability)) %>%
  left_join(siteinfo %>% filter(source == "ECOD")) %>% ungroup() %>% 
  mutate(DischargeReliability = as_factor(DischargeReliability),
         GageReliability = as_factor(GageReliability),
         TempReliability = as_factor(TempReliability))
# set tz to local and convert to UTC
tz(dat_flat$datetime) <- "UTC"
head(dat_flat)



# Greater Yellowstone/Al-Chokhachy
gyafiles <- list.files("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/Raw data/Al-Chokhachy/Al-Chokhachy data files")
gyalist <- list()
for (i in 1:length(gyafiles)) { 
  gyalist[[i]] <- read_csv(paste("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/Raw data/Al-Chokhachy/Al-Chokhachy data files/", gyafiles[i], sep = "")) %>%
    mutate(date = mdy(date), datetime = ymd_hms(paste(date, time, sep = " "), tz = "MST"), discharge = as.numeric(discharge)*35.314666212661) %>% 
    rename(DischargeReliability = DischargeReliability_JB, TempReliability = TempReliability_JB)
}
dat_gya <- bind_rows(gyalist) %>% select(datetime, depth, discharge, temperature, location, DischargeReliability, TempReliability) %>% 
  rename(height = depth, flow = discharge, tempc = temperature, site_name = location) %>%
  filter(site_name != "EF Henrys") %>% # drop weird duplicate site/year?
  mutate(site_name = dplyr::recode(site_name,
                            "EF Above Confluence" = "EF Duck Creek ab HF",
                            "EF Below Confluence" = "EF Duck Creek be HF",
                            "NF Spread Creek" = "NF Spread Creek Lower",
                            "Upper NF Spread Creek" = "NF Spread Creek Upper",
                            "SF Spread Creek" = "SF Spread Creek Lower",
                            "Upper SF Spread Creek" = "SF Spread Creek Upper",
                            "Shields River above Dugout Creek" = "Shields River ab Dugout",
                            "Upper Leidy Creek" = "Leidy Creek Upper", 
                            "Leidy Creek" = "Leidy Creek Mouth",
                            "Spread Creek" = "Spread Creek Dam",
                            "Shields River above Smith Creek" = "Shields River Valley Ranch")) %>%
  left_join(siteinfo %>% filter(source == "ECOD")) %>% filter(tempc <= 100) %>%
  mutate(DischargeReliability = as.factor(DischargeReliability), TempReliability = as.factor(TempReliability))
# set tz to local and convert to UTC
tz(dat_gya$datetime) <- "MST"
dat_gya$datetime <- with_tz(dat_gya$datetime, "UTC")
head(dat_gya)
```


Bind EcoD hourly flow/temp data with siteinfo and write to file
```{r}
dat <- bind_rows(dat_wb, dat_shen, dat_flat, dat_gya)
```


### Duplicates

Check for duplicates: number of duplicated unique date/times by site. For the Duck Ck and Spread Ck sites, this is driven by errors in how the datetimes are coded/specified (see ReviewData.qmd). 
```{r}
dat %>% group_by(site_name, datetime) %>% filter(n()>1) %>% arrange(site_name, datetime) %>% group_by(site_name) %>% summarize(num_dups = length(unique(datetime))) %>% kable()
```


### Check unique

Check unique designations
```{r}
unique(dat$designation)
```

Check unique regions
```{r}
unique(dat$region)
```

Check unique basins
```{r}
unique(dat$basin)
```

Check unique subbasins
```{r}
unique(dat$subbasin)
```

### Inspect raw data

*See ReviewData.qmd for site-level dyGraph plots of raw flow and temperature time series data (file size too large to include here).*


### Write to file
```{r}
write_csv(dat, "C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/EcoDrought_FlowTempData_Raw.csv")
dat <- read_csv("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/EcoDrought_FlowTempData_Raw.csv")
```


## NWIS data

### Download NWIS data

Download sub-daily flow and temp data from NWIS and ensure time zone = UTC
```{r eval=FALSE}
nwis_list <- list()
for (i in 1:length(sites)) {
  nwis_list[[i]] <- tibble(readNWISdata(sites = sites[i], service = "uv", parameterCd = c("00010", "00060"), 
                                        startDate = "1980-01-01", endDate = Sys.Date(), tz = "UTC"))
  print(i)
}
nwis_subdaily <- do.call(bind_rows, nwis_list)
nwis_subdaily <- nwis_subdaily %>% select(-1)
names(nwis_subdaily) <- c("station_no", "datetime", "flowcfs", "flowcfs_appcd", "tz", "tempc", "tempc_appcd")
nwis_subdaily <- nwis_subdaily %>% left_join(siteinfo %>% filter(source == "NWIS"))
write_csv(nwis_subdaily, "C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/EcoDrought_NWIS_FlowTempData_Raw_SubDaily.csv")
```

```{r}
nwis_subdaily <- read_csv("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/EcoDrought_NWIS_FlowTempData_Raw_SubDaily.csv") %>% 
  mutate(site_name = dplyr::recode(site_name, "Fish Creek" = "Fish Creek NWIS", "Avery Broook NWIS" = "Avery Brook NWIS")) %>% filter(grepl("NWIS", site_name))

tz(nwis_subdaily$datetime) <- "UTC"
nwis_subdaily$datetime <- with_tz(nwis_subdaily$datetime, "UTC")

head(nwis_subdaily)
```

### Check approval

* *A*, *A[91]*, *A[92]*, *A[93]*: approved and historical daily values ~match observed data
* *A e*: approved estimated data, often during ice affected periods (smoothed data)
* *P*: provisional data yet to be approved, typically these are just more recent observations
* *P e*: provisional estimated data, often during ice affected periods
* *P Ice*: ice affected observations
* *P dis*: provisional data yet to be approved, site has been discontinued
* *NA*: missing value, usually b/c temp is observed but flow is not

Flow approval codes
```{r}
unique(nwis_subdaily$flowcfs_appcd)
```
Temp approval codes
```{r}
unique(nwis_subdaily$tempc_appcd)
```

Check approval for key sites:

::: panel-tabset
#### West Brook
```{r, echo=FALSE}
nwis_subdaily %>% filter(site_name == "West Brook NWIS") %>% select(datetime, flowcfs, flowcfs_appcd) %>% spread(key = flowcfs_appcd, value = flowcfs) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "flow (cfs)")%>% dyOptions(drawGrid = FALSE, axisLineWidth = 1.5, fillGraph = TRUE)
```
#### Big Ck
```{r, echo=FALSE}
nwis_subdaily %>% filter(site_name == "Big Creek NWIS") %>% select(datetime, flowcfs, flowcfs_appcd) %>% spread(key = flowcfs_appcd, value = flowcfs) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "flow (cfs)")%>% dyOptions(drawGrid = FALSE, axisLineWidth = 1.5, fillGraph = TRUE)
```
#### Shields ab Smith
```{r, echo=FALSE}
nwis_subdaily %>% filter(site_name == "Shields River ab Smith NWIS") %>% select(datetime, flowcfs, flowcfs_appcd) %>% spread(key = flowcfs_appcd, value = flowcfs) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "flow (cfs)")%>% dyOptions(drawGrid = FALSE, axisLineWidth = 1.5, fillGraph = TRUE)
```
#### Donner-Blitzen
```{r, echo=FALSE}
nwis_subdaily %>% filter(site_name == "Donner Blitzen River nr Frenchglen NWIS", datetime >= as_date("2019-01-01")) %>% select(datetime, flowcfs, flowcfs_appcd) %>% spread(key = flowcfs_appcd, value = flowcfs) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "flow (cfs)")%>% dyOptions(drawGrid = FALSE, axisLineWidth = 1.5, fillGraph = TRUE)
```
#### SF Spread
```{r, echo=FALSE}
nwis_subdaily %>% filter(site_name == "SF Spread Creek Lower NWIS") %>% select(datetime, flowcfs, flowcfs_appcd) %>% spread(key = flowcfs_appcd, value = flowcfs) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "flow (cfs)")%>% dyOptions(drawGrid = FALSE, axisLineWidth = 1.5, fillGraph = TRUE)
```
:::


### View data

```{r, fig.height=8, fig.width=10}
#| fig-cap: "Stream flow (cfs) for Big/Super G NWIS gages"
nwis_subdaily %>% filter(designation == "big") %>% ggplot() + geom_line(aes(x = datetime, y = flowcfs)) + facet_wrap(~site_name, scales = "free_y")
```

```{r, fig.height=8, fig.width=10}
#| fig-cap: "Stream flow (cfs) for Medium G NWIS gages"
nwis_subdaily %>% filter(designation == "medium") %>% ggplot() + geom_line(aes(x = datetime, y = flowcfs)) + facet_wrap(~site_name, scales = "free_y" )
```

## Bind EcoD and NWIS

Bind EcoD and NWIS data, specify common reliability/approval codes. Also round date-times to nearest minute and summarize flow/temp data as loggers at some sites (e.g., in the Flathead) will take multiple readings within seconds of each other
```{r}
dat_full <- bind_rows(dat %>% mutate(tz = tz(datetime)), 
                      nwis_subdaily %>% 
                        rename(flow = flowcfs) %>%
                        mutate(DischargeReliability = ifelse(flowcfs_appcd %in% c("A [91]", "A [92]", "A [93]", "A"), 1, 0),
                               TempReliability = ifelse(tempc_appcd %in% c("A", "P", "P Ssn", "P Eqp", "P Dis"), 1, 0))) 
head(dat_full)
```


### Duplicates

Check for duplicates: number of duplicated unique date/times by site. For the Duck Ck and Spread Ck sites, this is driven by errors in how the datetimes are coded/specified (see ReviewData.qmd). 
```{r}
dat_full %>% group_by(site_name, datetime) %>% filter(n()>1) %>% arrange(site_name, datetime) %>% ungroup() %>% group_by(site_name) %>% summarize(num_dups = length(unique(datetime))) %>% kable()
```

### Write to file
```{r}
write_csv(dat_full, "C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/EcoDrought_FlowTempData_Raw_ECODandNWIS.csv")
```


## Calculate daily means

Get daily summaries for *DischargeReliability == 1* only! Duplicate Shields River Livingston data for Duck Creek big G.
```{r}
dat_daily <- dat_full %>% 
  mutate(date = as_date(datetime),
         flow = ifelse(DischargeReliability == 1, flow, NA),
         tempc = ifelse(TempReliability == 1, tempc, NA)) %>% 
  group_by(station_no, site_name, site_id, basin, subbasin, region, lat, long, elev_ft, area_sqmi, designation, date) %>% 
  summarize(DischargeReliability = max(DischargeReliability),
            TempReliability = max(TempReliability),
            flow_mean = mean(flow), flow_min = min(flow), flow_max = max(flow),
            tempc_mean = mean(tempc), tempc_min = min(tempc), tempc_max = max(tempc)) %>%
  arrange(region, basin, site_name, date) %>%
  ungroup()
dat_daily <- bind_rows(dat_daily, dat_daily %>% filter(site_id == "SRL") %>% mutate(subbasin = "Duck Creek")) 
head(dat_daily)
```

Add missing dates
```{r}
dat_daily <- fill_missing_dates(dat_daily, dates = date, groups = site_name)
```


View daily mean time series data, for example, site_name = CoalCreekLower. Notice the many shorts gaps in the daily data.
```{r}
dat_daily %>% filter(site_name == "CoalCreekLower") %>% select(date, flow_mean) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "Mean daily flow (cfs)") %>% dyOptions(fillGraph = TRUE, drawGrid = FALSE, axisLineWidth = 1.5)
```

Check for duplicates
```{r}
dups <- dat_daily %>% group_by(site_name, date) %>% filter(n()>1) %>% arrange(site_name, date)
nrow(dups)/2
unique(dups$site_name)
```


## Interpolate missing data

Small periods of missing data (\<24 hours) become a problem when aggregating at the daily and weekly time scales. \*Note that currently this discovers and fills missing data at the daily time scale, but should be changed to interpolate at the original timescale of the raw data (e.g., hourly).
```{r}
# explore data gaps
mysites <- unique(dat_daily$site_name)
mynas <- list()
for (i in 1:length(mysites)) {
  mydisch <- unlist(dat_daily$flow_mean[dat_daily$site_name == mysites[i]])
  runsna <- rle(is.na(mydisch))
  mynas[[i]] <- tibble(site_name = mysites[i], run = runsna$lengths[runsna$values == TRUE])
}
mynas <- do.call(rbind, mynas)
```

Most gaps are relatively short
```{r}
#| fig-cap: "Distributiion of lengths of missing data (days)"
mynas %>% ggplot() + geom_histogram(aes(x = run)) + xlab("Days") + ylab("Frequency")
```

Zoomed in...
```{r}
#| fig-cap: "Distributiion of lengths of missing data (days)"
mynas %>% ggplot() + geom_histogram(aes(x = run))  + xlab("Days") + ylab("Frequency") + xlim(0,30)
```

Considering just the short gaps, which are likely most commonly a function of ice effects, which sites are problematic?
```{r fig.width=9, fig.height=5}
#| fig-cap: "Frequency of short (<40 days) data gaps by site"
mynas %>% filter(run <= 40) %>% select(site_name) %>% ggplot() + geom_bar(aes(x = site_name)) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

Fill short gaps (<=7 days) using time series interpolation
```{r}
datalist <- list()
for (i in 1:length(mysites)) { datalist[[i]] <- dat_daily %>% filter(site_name == mysites[i]) %>% mutate(flow_mean_filled = fillMissing(flow_mean, max.fill = 7, span = 100)) }
dat_daily_fill <- do.call(rbind, datalist)
```

Explore interpolated/filled time series relative to original (daily) data Again, CoalCreekLower as an example. 
```{r}
dat_daily_fill %>% filter(site_name == "CoalCreekLower") %>% select(date, flow_mean, flow_mean_filled) %>% dygraph() %>% dyRangeSelector() %>% dySeries("flow_mean", strokeWidth = 5, color = "black") %>% dySeries("flow_mean_filled", strokeWidth = 1, color = "red") %>% dyAxis("y", label = "Mean daily flow (cfs)") %>% dyOptions(fillGraph = TRUE, drawGrid = FALSE, axisLineWidth = 1.5)
```

## Calculate yield
```{r}
# convert cfs and basin area to metric
dat_daily_fill <- dat_daily_fill %>% mutate(flow_mean_cms = flow_mean*0.02831683199881, 
                                            flow_mean_filled_cms = flow_mean_filled*0.02831683199881, 
                                            area_sqkm = area_sqmi*2.58999)

# sites
sites <- unique(dat_daily_fill$site_name)

# site-specific basin area in square km
basinarea <- dat_daily_fill %>% filter(!is.na(site_id)) %>% group_by(site_name) %>% summarize(area_sqkm = unique(area_sqkm))

# calculate yield
yield_list <- list()
for (i in 1:length(sites)) {
  d <- dat_daily_fill %>% filter(site_name == sites[i])
  ba <- unlist(basinarea %>% filter(site_name == sites[i]) %>% select(area_sqkm))
  yield_list[[i]] <-  add_daily_yield(data = d, values = flow_mean_cms, basin_area = as.numeric(ba)) %>% left_join(add_daily_yield(data = d, values = flow_mean_filled_cms, basin_area = as.numeric(ba)) %>% rename(Yield_filled_mm = Yield_mm))
}
dat_daily_fill_wyield <- do.call(rbind, yield_list)
```

## Calculate 7-day means

```{r}
dat_daily_fill_wyield <- dat_daily_fill_wyield %>%
  group_by(site_name) %>%
  mutate(flow_mean_7 = rollapply(flow_mean, FUN = mean, width = 7, align = "center", fill = NA),
         flow_mean_filled_7 = rollapply(flow_mean_filled, FUN = mean, width = 7, align = "center", fill = NA),
         tempc_mean_7 = rollapply(tempc_mean, FUN = mean, width = 7, align = "center", fill = NA),
         Yield_mm_7 = rollapply(Yield_mm, FUN = mean, width = 7, align = "center", fill = NA),
         Yield_filled_mm_7 = rollapply(Yield_filled_mm, FUN = mean, width = 7, align = "center", fill = NA)) %>%
  ungroup() %>% filter(!is.na(site_id))
```


## View daily data

View daily time series data by sub-basin (little and medium g's only)

::: panel-tabset
#### Big Creek
```{r, echo=FALSE}
dat_daily_fill_wyield %>% filter(designation %in% c("little", "medium"), subbasin == "Big Creek") %>% mutate(logYield = log(Yield_filled_mm)) %>% select(date, site_name, logYield) %>% spread(key = site_name, value = logYield) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "ln(Yield, mm)")
```
#### Coal Creek
```{r, echo=FALSE}
dat_daily_fill_wyield %>% filter(designation %in% c("little", "medium"), subbasin == "Coal Creek") %>% mutate(logYield = log(Yield_filled_mm)) %>% select(date, site_name, logYield) %>% spread(key = site_name, value = logYield) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "ln(Yield, mm)")
```
#### McGee Creek
```{r, echo=FALSE}
dat_daily_fill_wyield %>% filter(designation %in% c("little", "medium"), subbasin == "McGee Creek") %>% mutate(logYield = log(Yield_filled_mm)) %>% select(date, site_name, logYield) %>% spread(key = site_name, value = logYield) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "ln(Yield, mm)")
```
#### West Brook
```{r, echo=FALSE}
dat_daily_fill_wyield %>% filter(designation %in% c("little", "medium"), subbasin == "West Brook") %>% mutate(logYield = log(Yield_filled_mm)) %>% select(date, site_name, logYield) %>% spread(key = site_name, value = logYield) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "ln(Yield, mm)")
```
#### Paine Run
```{r, echo=FALSE}
dat_daily_fill_wyield %>% filter(designation %in% c("little", "medium"), subbasin == "Paine Run") %>% mutate(logYield = log(Yield_filled_mm)) %>% select(date, site_name, logYield) %>% spread(key = site_name, value = logYield) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "ln(Yield, mm)")
```
#### Staunton River
```{r, echo=FALSE}
dat_daily_fill_wyield %>% filter(designation %in% c("little", "medium"), subbasin == "Staunton River") %>% mutate(logYield = log(Yield_filled_mm)) %>% select(date, site_name, logYield) %>% spread(key = site_name, value = logYield) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "ln(Yield, mm)")
```
#### Duck Creek
```{r, echo=FALSE}
dat_daily_fill_wyield %>% filter(designation %in% c("little", "medium"), subbasin == "Duck Creek") %>% mutate(logYield = log(Yield_filled_mm)) %>% select(date, site_name, logYield) %>% spread(key = site_name, value = logYield) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "ln(Yield, mm)")
```
#### Shields River
```{r, echo=FALSE}
dat_daily_fill_wyield %>% filter(designation %in% c("little", "medium"), subbasin == "Shields River") %>% mutate(logYield = log(Yield_filled_mm)) %>% select(date, site_name, logYield) %>% spread(key = site_name, value = logYield) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "ln(Yield, mm)")
```
#### Snake River
```{r, echo=FALSE}
dat_daily_fill_wyield %>% filter(designation %in% c("little", "medium"), subbasin == "Snake River") %>% mutate(logYield = log(Yield_filled_mm)) %>% select(date, site_name, logYield) %>% spread(key = site_name, value = logYield) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "ln(Yield, mm)")
```
#### Donner Blitzen
```{r, echo=FALSE}
dat_daily_fill_wyield %>% filter(designation %in% c("little", "medium"), subbasin == "Donner Blitzen") %>% mutate(logYield = log(Yield_filled_mm)) %>% select(date, site_name, logYield) %>% spread(key = site_name, value = logYield) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "ln(Yield, mm)")
```
:::


## Write out and read data

```{r}
# write out
write_csv(dat_daily_fill_wyield, "C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/EcoDrought_FlowTempData_DailyWeekly.csv")
dat_daily <- read_csv("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/Data/EcoDrought_FlowTempData_DailyWeekly.csv")
```

## View data availability

```{r}
# summarize data availability
dat_summ <- dat_daily %>% filter(site_id != "MRN") %>%
  group_by(basin, date, designation) %>% summarize(numflow = sum(!is.na(flow_mean)), numtemp = sum(!is.na(tempc_mean))) %>% 
  gather(type, avail, numflow:numtemp) %>% mutate(type2 = as.factor(paste(designation, type, sep = "_"))) %>% 
  mutate(type3 = as.numeric(type2), avail = na_if(avail, 0)) %>% ungroup() %>% filter(!is.na(avail))
```

```{r, fig.height = 6, fig.width = 12}
# plot all years
#| fig-cap: "Data availability by basin, all years"
myplot <- dat_summ %>% ggplot(aes(x = date, y = type3)) + 
  geom_errorbarh(aes(xmax = date, xmin = date, color = avail), size = 0.001) +
  scale_color_continuous(trans = "reverse", na.value = "grey60") +
  scale_y_discrete(limits = c("Big G flow", "Big G temp", "Medium g flow", "Medium g temp", "Little g flow", "Little g temp")) + 
  labs(colour = "Number \nof sites") + ylab("") + xlab("Date") + 
  facet_wrap(~ basin, nrow = 4,
             labeller = as_labeller(c(`West Brook` = "West Brook: G = South River Conway (NWIS)", 
                                      `Staunton River` = "Staunton River: G = Staunton River 10 (UVA)",
                                      `Paine Run` = "Paine Run: G = Paine Run 10 (UVA)",
                                      `Piney River` = "Piney River: G = Piney River 10 (UVA)",
                                      `Snake River` = "Snake River: G = Pacific Creek Moran (NWIS)",
                                      `Shields River` = "Shields River: G = Shields River Livingston (NWIS)",
                                      `Flathead` = "NF Flathead River: G = NF Flathead (NWIS)",
                                      `Donner Blitzen` = "Donner Blitzen River: G = DB Frenchglen (NWIS)",
                                      `Duck Creek` = "Duck Creek: G = Shields River Livingston (NWIS)")))
myplot
# jpeg("C:/Users/jbaldock/OneDrive - DOI/Documents/USGS/EcoDrought/EcoDrought Working/EcoDrought-Analysis/Data Availability/DataAvailability_byBasin.jpg", height = 6, width = 12, units = "in", res = 500)
# myplot
# dev.off()
```

```{r, fig.height = 6, fig.width = 12}
# plot recent years
#| fig-cap: "Data availability by basin, recent years"
# jpeg("./Data Availability/DataAvailability_byBasin_recent.jpg", height = 6, width = 12, units = "in", res = 500)
myplot <- dat_summ %>% filter(date >= "2018-10-01") %>% ggplot(aes(x = date, y = type3)) + 
  geom_errorbarh(aes(xmax = date, xmin = date, color = avail), size = 0.001) +
  scale_color_continuous(trans = "reverse", na.value = "grey60") +
  scale_y_discrete(limits = c("Big G flow", "Big G temp", "Medium g flow", "Medium g temp", "Little g flow", "Little g temp")) + 
  labs(colour = "Number \nof sites") + ylab("") + xlab("Date") + 
  facet_wrap(~ basin, nrow = 4,
             labeller = as_labeller(c(`West Brook` = "West Brook: G = South River Conway (NWIS)", 
                                      `Staunton River` = "Staunton River: G = Staunton River 10 (UVA)",
                                      `Paine Run` = "Paine Run: G = Paine Run 10 (UVA)",
                                      `Piney River` = "Piney River: G = Piney River 10 (UVA)",
                                      `Snake River` = "Snake River: G = Pacific Creek Moran (NWIS)",
                                      `Shields River` = "Shields River: G = Shields River Livingston (NWIS)",
                                      `Flathead` = "NF Flathead River: G = NF Flathead (NWIS)",
                                      `Donner Blitzen` = "Donner Blitzen River: G = DB Frenchglen (NWIS)",
                                      `Duck Creek` = "Duck Creek: G = Shields River Livingston (NWIS)")))
myplot
```

## Compare co-located gages

### Compare synchronous gages
Compare streamflow data from co-located EcoDrought and NWIS gages with overlapping periods of record
```{r, fig.height = 6, fig.width = 6}
#| fig-cap: "Streamflow measured at little g gages (EcoDrought) as a function of streamflow measured at medium g gages (NWIS), on a log-scale. Red line denotes 1:1"
# WEST BROOK
p1 <- dat_daily %>% filter(site_name == "West Brook 0") %>% select(date, flow_mean_7) %>% rename(little = flow_mean_7) %>% 
  left_join(dat_daily %>% filter(site_name == "West Brook NWIS") %>% select(date, flow_mean_7) %>% rename(medium = flow_mean_7)) %>%
  ggplot() + geom_point(aes(x = log(medium), y = log(little))) + geom_abline(intercept = 0, slope = 1, color = "red")

p2 <- dat_daily %>% filter(site_name == "Avery Brook") %>% select(date, flow_mean_7) %>% rename(little = flow_mean_7) %>% 
  left_join(dat_daily %>% filter(site_name == "Avery Broook NWIS") %>% select(date, flow_mean_7) %>% rename(medium = flow_mean_7)) %>%
  ggplot() + geom_point(aes(x = log(medium), y = log(little))) + geom_abline(intercept = 0, slope = 1, color = "red")

# FLATHEAD
p3 <- dat_daily %>% filter(site_name == "BigCreekMiddle") %>% select(date, flow_mean_7) %>% rename(little = flow_mean_7) %>% 
  left_join(dat_daily %>% filter(site_name == "Big Creek NWIS") %>% select(date, flow_mean_7) %>% rename(medium = flow_mean_7)) %>%
  ggplot() + geom_point(aes(x = log(medium), y = log(little))) + geom_abline(intercept = 0, slope = 1, color = "red")

# jpeg("./Data Availability/LittleMedium_Co-Located_Gages.jpg", height = 6, width = 6, units = "in", res = 500)
ggarrange(p1, p2, p3, ncol = 2, nrow = 2, labels = c("West Brook 0", "Avery Brook", "Big Creek (Flathead)"))
# dev.off()
```

### Compare asynchronous gages
For Spread Creek and Shields River, compare data from co-located EcoDrought and NWIS gages, with NON-overlapping periods of record. Note that streamflow from NWIS gages is about ~1 order of magnitude greater than what is measured at EcoD gages.

::: panel-tabset
#### Leidy Creek
```{r, echo=FALSE}
dat_daily %>% filter(site_name %in% c("Leidy Creek Mouth", "Leidy Creek Mouth NWIS")) %>% mutate(logflow = log(flow_mean_filled_7)) %>% select(date, site_name, logflow) %>% spread(key = site_name, value = logflow) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "ln(7-day mean flow, cfs)")
```
#### SF Spread Creek Lower
```{r, echo=FALSE}
dat_daily %>% filter(site_name %in% c("SF Spread Creek Lower", "SF Spread Creek Lower NWIS")) %>% mutate(logflow = log(flow_mean_filled_7)) %>% select(date, site_name, logflow) %>% spread(key = site_name, value = logflow) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "ln(7-day mean flow, cfs)")
```
#### Dugout Creek
```{r, echo=FALSE}
dat_daily %>% filter(site_name %in% c("Dugout Creek", "Dugout Creek NWIS")) %>% mutate(logflow = log(flow_mean_filled_7)) %>% select(date, site_name, logflow) %>% spread(key = site_name, value = logflow) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "ln(7-day mean flow, cfs)")
```
#### Sheilds Valley Ranch
```{r, echo=FALSE}
dat_daily %>% filter(site_name %in% c("Shields River Valley Ranch", "Shields River ab Smith NWIS")) %>% mutate(logflow = log(flow_mean_filled_7)) %>% select(date, site_name, logflow) %>% spread(key = site_name, value = logflow) %>% dygraph() %>% dyRangeSelector() %>% dyAxis("y", label = "ln(7-day mean flow, cfs)")
```
:::


